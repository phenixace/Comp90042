{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from dataset import Dataset4SKEP\r\n",
    "import paddle\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "import paddle.nn as nn\r\n",
    "\r\n",
    "train_ds = Dataset4SKEP('train')\r\n",
    "train_ds = MapDataset(train_ds)\r\n",
    "dev_ds   = Dataset4SKEP('dev')\r\n",
    "dev_ds = MapDataset(dev_ds)\r\n",
    "test_ds  = Dataset4SKEP('test')\r\n",
    "test_ds = MapDataset(test_ds)\r\n",
    "print(train_ds[0:2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'text': '5. Can regularly rinsing your nose with saline help prevent infection with the new coronavirus? 4. Can eating garlic help prevent infection with the new coronavirus? 6. Do vaccines against pneumonia protect you against the new coronavirus? 7. Can spraying alcohol or chlorine all over your body kill the new coronavirus? 8. How effective are thermal scanners in detecting people infected with the new coronavirus? 9. Can an ultraviolet disinfection lamp kill the new coronavirus? 10. Are hand dryers effective in killing the new coronavirus? 11. The new coronavirus CANNOT be transmitted through mosquito bites. 12. Taking a hot bath does not prevent the new coronavirus disease 13. Cold weather and snow CANNOT kill the new coronavirus. 14. COVID19 virus can be transmitted in areas with hot and humid climates 15. Drinking alcohol does not protect you against COVID19 and can be dangerous 16. Being able to hold your breath for 10 seconds or more without coughing or feeling discomfort DOES NOT mean you 17. You can recover from the coronavirus disease COVID19. Catching the new coronavirus DOES NOT mean you will ha 18. Exposing yourself to the sun or to temperatures higher than 25C degrees DOES NOT prevent the coronavirus diseas 19. 5G mobile networks DO NOT spread COVID19', 'label': 0, 'qid': 0}, {'text': \"French police chief killed himself after attack Devastating. RT French police chief killed himself after attack Nothing to see here. Move along. it stinkS !!! These whole affaire!!! And tasked with investigating Charlie Hebdo shooting. But 'obviously' just a strange coincidence. Not like he discovered anything FWIW, his 2nd in command comitted suicide in 2013, amp he found the body. So, there's that. P Ohh boy,Here v go Pakistanis already calling it false flag by Eivil JewsMT French police chief killed himself after attack wtf? Why? Yeh sure he did... why are you reporting this three days after it happened? those cartoonistS are responsible, those were sent to hell,by humiliating norms of Islam and blasphemy of PBUH OF ISLAM. so horrible. They killed him how can he kill himself? was shot in the forehead! so taht he doesnt get who's behind it WHAT YA WANNA BET HE WAS KILLED?????????? took it kinda personal I guess, sad Don't worry...it's all another weird coincidence. No conspiracy here. such a sad loss may he now RIP How very sad. The trauma he must have faced. Very sad. French police chief killed himself after attack Terrible! oh no... we will be very satisfied if resigns. Here they hide the reports which blames them. horrible!!! Needless. Did Dubya kill himself after 911 or just go braindead...\", 'label': 1, 'qid': 1}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\r\n",
    "\r\n",
    "# load skep\r\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\", num_classes=2)\r\n",
    "# tokenizer loaded\r\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[2022-04-28 21:33:17,459] [    INFO]\u001b[0m - Already cached C:\\Users\\46901\\.paddlenlp\\models\\skep_ernie_2.0_large_en\\skep_ernie_2.0_large_en.pdparams\u001b[0m\n",
      "\u001b[32m[2022-04-28 21:33:28,492] [    INFO]\u001b[0m - Already cached C:\\Users\\46901\\.paddlenlp\\models\\skep_ernie_2.0_large_en\\skep_ernie_2.0_large_en.vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader,convert_example\r\n",
    "\r\n",
    "\r\n",
    "batch_size = 4\r\n",
    "max_seq_length = 512\r\n",
    "\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack()  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "\r\n",
    "\r\n",
    "import time\r\n",
    "\r\n",
    "from metrics import evaluate\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "# save_dir\r\n",
    "ckpt_dir = \"skep_ckpt\"\r\n",
    "# step number\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=2e-5,\r\n",
    "    parameters=model.parameters())\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "metric = paddle.metric.Accuracy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print('Total: ',num_training_steps)\r\n",
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 200 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 200 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "\r\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\r\n",
    "\r\n",
    "            model.save_pretrained(save_dir)\r\n",
    "\r\n",
    "            tokenizer.save_pretrained(save_dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total:  4540\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.10253, accu: 0.78250, speed: 0.17 step/s\n",
      "eval loss: 0.29538, accu: 0.87899,  F1: 0.8097, Precision: 0.8587, Recall: 0.7807\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.02642, accu: 0.87250, speed: 0.13 step/s\n",
      "eval loss: 0.20402, accu: 0.91261,  F1: 0.8786, Precision: 0.8732, Recall: 0.8844\n",
      "global step 600, epoch: 2, batch: 146, loss: 0.06386, accu: 0.92472, speed: 0.12 step/s\n",
      "eval loss: 0.17779, accu: 0.93782,  F1: 0.9134, Precision: 0.9084, Recall: 0.9187\n",
      "global step 800, epoch: 2, batch: 346, loss: 0.08065, accu: 0.94500, speed: 0.12 step/s\n",
      "eval loss: 0.15562, accu: 0.94286,  F1: 0.9186, Precision: 0.9230, Recall: 0.9143\n",
      "global step 1000, epoch: 3, batch: 92, loss: 0.01235, accu: 0.97491, speed: 0.12 step/s\n",
      "eval loss: 0.14398, accu: 0.95126,  F1: 0.9314, Precision: 0.9303, Recall: 0.9325\n",
      "global step 1200, epoch: 3, batch: 292, loss: 0.00579, accu: 0.97500, speed: 0.11 step/s\n",
      "eval loss: 0.15965, accu: 0.94286,  F1: 0.9124, Precision: 0.9579, Recall: 0.8810\n",
      "global step 1400, epoch: 4, batch: 38, loss: 0.04112, accu: 0.95859, speed: 0.12 step/s\n",
      "eval loss: 0.17829, accu: 0.94454,  F1: 0.9190, Precision: 0.9353, Recall: 0.9051\n",
      "global step 1600, epoch: 4, batch: 238, loss: 0.00319, accu: 0.96750, speed: 0.12 step/s\n",
      "eval loss: 0.15759, accu: 0.94622,  F1: 0.9245, Precision: 0.9224, Recall: 0.9267\n",
      "global step 1800, epoch: 4, batch: 438, loss: 0.02646, accu: 0.97375, speed: 0.12 step/s\n",
      "eval loss: 0.10229, accu: 0.95798,  F1: 0.9426, Precision: 0.9299, Recall: 0.9574\n",
      "global step 2000, epoch: 5, batch: 184, loss: 0.00079, accu: 0.98996, speed: 0.12 step/s\n",
      "eval loss: 0.13439, accu: 0.95798,  F1: 0.9412, Precision: 0.9379, Recall: 0.9446\n",
      "global step 2200, epoch: 5, batch: 384, loss: 0.00180, accu: 0.98625, speed: 0.11 step/s\n",
      "eval loss: 0.11912, accu: 0.95462,  F1: 0.9380, Precision: 0.9255, Recall: 0.9526\n",
      "global step 2400, epoch: 6, batch: 130, loss: 0.00094, accu: 0.99624, speed: 0.11 step/s\n",
      "eval loss: 0.16899, accu: 0.96639,  F1: 0.9526, Precision: 0.9526, Recall: 0.9526\n",
      "global step 2600, epoch: 6, batch: 330, loss: 0.00022, accu: 0.99375, speed: 0.12 step/s\n",
      "eval loss: 0.18729, accu: 0.95462,  F1: 0.9389, Precision: 0.9217, Recall: 0.9603\n",
      "global step 2800, epoch: 7, batch: 76, loss: 0.00117, accu: 0.99247, speed: 0.12 step/s\n",
      "eval loss: 0.30832, accu: 0.94454,  F1: 0.9263, Precision: 0.9059, Recall: 0.9537\n",
      "global step 3000, epoch: 7, batch: 276, loss: 0.00096, accu: 0.99250, speed: 0.12 step/s\n",
      "eval loss: 0.16673, accu: 0.95630,  F1: 0.9380, Precision: 0.9403, Recall: 0.9358\n",
      "global step 3200, epoch: 8, batch: 22, loss: 0.00048, accu: 0.98745, speed: 0.12 step/s\n",
      "eval loss: 0.18036, accu: 0.95294,  F1: 0.9333, Precision: 0.9355, Recall: 0.9311\n",
      "global step 3400, epoch: 8, batch: 222, loss: 0.00237, accu: 0.98875, speed: 0.12 step/s\n",
      "eval loss: 0.22657, accu: 0.94622,  F1: 0.9267, Precision: 0.9137, Recall: 0.9420\n",
      "global step 3600, epoch: 8, batch: 422, loss: 0.00168, accu: 0.98375, speed: 0.11 step/s\n",
      "eval loss: 0.25425, accu: 0.94622,  F1: 0.9260, Precision: 0.9163, Recall: 0.9369\n",
      "global step 3800, epoch: 9, batch: 168, loss: 0.00693, accu: 0.99498, speed: 0.12 step/s\n",
      "eval loss: 0.29066, accu: 0.90924,  F1: 0.8855, Precision: 0.8585, Recall: 0.9359\n",
      "global step 4000, epoch: 9, batch: 368, loss: 0.00044, accu: 0.99750, speed: 0.12 step/s\n",
      "eval loss: 0.19116, accu: 0.94958,  F1: 0.9319, Precision: 0.9158, Recall: 0.9519\n",
      "global step 4200, epoch: 10, batch: 114, loss: 0.00094, accu: 1.00000, speed: 0.12 step/s\n",
      "eval loss: 0.26104, accu: 0.95126,  F1: 0.9337, Precision: 0.9197, Recall: 0.9504\n",
      "global step 4400, epoch: 10, batch: 314, loss: 0.00108, accu: 1.00000, speed: 0.11 step/s\n",
      "eval loss: 0.21496, accu: 0.96134,  F1: 0.9456, Precision: 0.9445, Recall: 0.9468\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\r\n",
    "from utils import convert_example\r\n",
    "\r\n",
    "batch_size = 2 # limited by the memory of 3080\r\n",
    "# process test data\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length,\r\n",
    "    is_test=True)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\r\n",
    "    Stack() # qid\r\n",
    "): [data for data in fn(samples)]\r\n",
    "test_data_loader = create_dataloader(\r\n",
    "    test_ds,\r\n",
    "    mode='test',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# choose model directory\r\n",
    "params_path = './skep_ckpt//model_320/model_state.pdparams'\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # load model\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)\r\n",
    "else:\r\n",
    "    print(\"Model not found\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded parameters from ./skep_ckpt//model_3200/model_state.pdparams\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "results = []\r\n",
    "\r\n",
    "model.eval()\r\n",
    "for batch in test_data_loader:\r\n",
    "    input_ids, token_type_ids, qids = batch\r\n",
    "\r\n",
    "    logits = model(input_ids, token_type_ids)\r\n",
    "    probs = F.softmax(logits, axis=-1)\r\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "    idx = idx.tolist()\r\n",
    "    labels = [str(i) for i in idx]\r\n",
    "    qids = qids.numpy().tolist()\r\n",
    "    results.extend(zip(qids, labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "res_dir = \"./results\"\r\n",
    "if not os.path.exists(res_dir):\r\n",
    "    os.makedirs(res_dir)\r\n",
    "\r\n",
    "with open(os.path.join(res_dir, \"SKEP.csv\"), 'w+', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"Id,Predicted\\n\")\r\n",
    "    for qid, label in results:\r\n",
    "        f.write(str(qid[0])+\",\"+label+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}