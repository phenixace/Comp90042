{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dataset import Dataset4SKEP\r\n",
    "import paddle\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "import paddle.nn as nn\r\n",
    "\r\n",
    "train_ds = Dataset4SKEP('train')\r\n",
    "train_ds = MapDataset(train_ds)\r\n",
    "dev_ds   = Dataset4SKEP('dev')\r\n",
    "dev_ds = MapDataset(dev_ds)\r\n",
    "test_ds  = Dataset4SKEP('test')\r\n",
    "test_ds = MapDataset(test_ds)\r\n",
    "print(train_ds[0:2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\r\n",
    "\r\n",
    "# load skep\r\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\", num_classes=2)\r\n",
    "# tokenizer loaded\r\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader,convert_example\r\n",
    "\r\n",
    "\r\n",
    "batch_size = 4\r\n",
    "max_seq_length = 512\r\n",
    "\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack()  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "\r\n",
    "\r\n",
    "import time\r\n",
    "\r\n",
    "from metrics import evaluate\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "# save_dir\r\n",
    "ckpt_dir = \"skep_ckpt\"\r\n",
    "# step number\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=2e-5,\r\n",
    "    parameters=model.parameters())\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "metric = paddle.metric.Accuracy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Total: ',num_training_steps)\r\n",
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 200 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 200 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "\r\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\r\n",
    "\r\n",
    "            model.save_pretrained(save_dir)\r\n",
    "\r\n",
    "            tokenizer.save_pretrained(save_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "from utils import convert_example\r\n",
    "\r\n",
    "batch_size = 2 # limited by the memory of 3080\r\n",
    "# process test data\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length,\r\n",
    "    is_test=True)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\r\n",
    "    Stack() # qid\r\n",
    "): [data for data in fn(samples)]\r\n",
    "test_data_loader = create_dataloader(\r\n",
    "    test_ds,\r\n",
    "    mode='test',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# choose model directory\r\n",
    "params_path = './skep_ckpt//model_320/model_state.pdparams'\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # load model\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)\r\n",
    "else:\r\n",
    "    print(\"Model not found\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = []\r\n",
    "\r\n",
    "model.eval()\r\n",
    "for batch in test_data_loader:\r\n",
    "    input_ids, token_type_ids, qids = batch\r\n",
    "\r\n",
    "    logits = model(input_ids, token_type_ids)\r\n",
    "    probs = F.softmax(logits, axis=-1)\r\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "    idx = idx.tolist()\r\n",
    "    labels = [str(i) for i in idx]\r\n",
    "    qids = qids.numpy().tolist()\r\n",
    "    results.extend(zip(qids, labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_dir = \"./results\"\r\n",
    "if not os.path.exists(res_dir):\r\n",
    "    os.makedirs(res_dir)\r\n",
    "\r\n",
    "with open(os.path.join(res_dir, \"SKEP.csv\"), 'w+', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"Id,Predicted\\n\")\r\n",
    "    for qid, label in results:\r\n",
    "        f.write(str(qid[0])+\",\"+label+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}